{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2021-09-28T05:12:12.891212Z",
     "iopub.status.busy": "2021-09-28T05:12:12.890581Z",
     "iopub.status.idle": "2021-09-28T05:12:19.610493Z",
     "shell.execute_reply": "2021-09-28T05:12:19.609587Z",
     "shell.execute_reply.started": "2021-09-28T05:12:12.890996Z"
    }
   },
   "outputs": [],
   "source": [
    "GCS_PATH=\"/backup/recog/clean/tfrecords/\"\n",
    "#-------------------\n",
    "# fixed params\n",
    "#------------------\n",
    "img_height  =  64\n",
    "img_width   =  512\n",
    "nb_channels =  3\n",
    "pos_max     =  100          \n",
    "enc_filters =  1024\n",
    "factor      =  32\n",
    "# calculated\n",
    "enc_shape   =(img_height//factor,img_width//factor, enc_filters )\n",
    "attn_shape  =(None, enc_filters )\n",
    "mask_len    =int((img_width//factor)*(img_height//factor))\n",
    "#----------------\n",
    "# imports\n",
    "#---------------\n",
    "import tensorflow as tf\n",
    "import random\n",
    "import json\n",
    "import os\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "#from kaggle_datasets import KaggleDatasets\n",
    "from glob import glob\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "%matplotlib inline\n",
    "#-------------\n",
    "# config-globals\n",
    "#-------------\n",
    "with open('vocab.json') as f:\n",
    "    vocab = json.load(f)[\"grapheme\"]\n",
    "    \n",
    "start_end=len(vocab)\n",
    "pad_value=start_end+1    \n",
    "\n",
    "print(\"Label len:\",pos_max)\n",
    "print(\"Vocab len:\",len(vocab))\n",
    "print(\"Start End:\",start_end)\n",
    "print(\"pad_value:\",pad_value)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-09-28T05:13:42.917254Z",
     "iopub.status.busy": "2021-09-28T05:13:42.916929Z",
     "iopub.status.idle": "2021-09-28T05:13:48.562928Z",
     "shell.execute_reply": "2021-09-28T05:13:48.561973Z",
     "shell.execute_reply.started": "2021-09-28T05:13:42.917223Z"
    }
   },
   "outputs": [],
   "source": [
    "#----------------------------------------------------------\n",
    "# Detect hardware, return appropriate distribution strategy\n",
    "#----------------------------------------------------------\n",
    "# TPU detection. No parameters necessary if TPU_NAME environment variable is set. On Kaggle this is always the case.\n",
    "try:\n",
    "    tpu = tf.distribute.cluster_resolver.TPUClusterResolver()  \n",
    "    print('Running on TPU ', tpu.master())\n",
    "except ValueError:\n",
    "    tpu = None\n",
    "\n",
    "if tpu:\n",
    "    tf.config.experimental_connect_to_cluster(tpu)\n",
    "    tf.tpu.experimental.initialize_tpu_system(tpu)\n",
    "    strategy = tf.distribute.experimental.TPUStrategy(tpu)\n",
    "    tf.config.optimizer.set_jit(True)\n",
    "else:\n",
    "    strategy = tf.distribute.get_strategy() \n",
    "    # default distribution strategy in Tensorflow. Works on CPU and single GPU.\n",
    "\n",
    "print(\"REPLICAS: \", strategy.num_replicas_in_sync)\n",
    "\n",
    "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "print(gpus)\n",
    "for gpu in gpus:\n",
    "    tf.config.experimental.set_memory_growth(gpu, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-09-28T05:13:48.564894Z",
     "iopub.status.busy": "2021-09-28T05:13:48.564647Z",
     "iopub.status.idle": "2021-09-28T05:14:06.626604Z",
     "shell.execute_reply": "2021-09-28T05:14:06.625503Z",
     "shell.execute_reply.started": "2021-09-28T05:13:48.564866Z"
    }
   },
   "outputs": [],
   "source": [
    "#--------------------------\n",
    "# GCS Paths and tfrecords\n",
    "#-------------------------\n",
    "def get_tfrecs(_path):\n",
    "    gcs_pattern=os.path.join(_path,'*.tfrecord')\n",
    "    file_paths = tf.io.gfile.glob(gcs_pattern)\n",
    "    random.shuffle(file_paths)\n",
    "    return file_paths\n",
    "    \n",
    "# get records and split\n",
    "#GCS_PATH = KaggleDatasets().get_gcs_path('aps-nid-recog-data')\n",
    "\n",
    "recs=get_tfrecs(GCS_PATH)\n",
    "random.shuffle(recs)\n",
    "\n",
    "eval_recs=recs[:2]\n",
    "train_recs=recs[2:]\n",
    "nb_train=len(train_recs)*10240\n",
    "nb_eval =len(eval_recs)*10240"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-09-28T05:14:06.628599Z",
     "iopub.status.busy": "2021-09-28T05:14:06.628138Z",
     "iopub.status.idle": "2021-09-28T05:14:06.637198Z",
     "shell.execute_reply": "2021-09-28T05:14:06.635884Z",
     "shell.execute_reply.started": "2021-09-28T05:14:06.628554Z"
    }
   },
   "outputs": [],
   "source": [
    "#-------------------------------------\n",
    "# batching , strategy and steps\n",
    "#-------------------------------------\n",
    "if strategy.num_replicas_in_sync==1:\n",
    "    BATCH_SIZE = 64\n",
    "else:\n",
    "    BATCH_SIZE = 16 * strategy.num_replicas_in_sync\n",
    "\n",
    "# set    \n",
    "STEPS_PER_EPOCH = nb_train//BATCH_SIZE\n",
    "EVAL_STEPS      = nb_eval//BATCH_SIZE\n",
    "print(\"Steps:\",STEPS_PER_EPOCH)\n",
    "print(\"Batch Size:\",BATCH_SIZE)\n",
    "print(\"Eval Steps:\",EVAL_STEPS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-09-28T05:14:51.121482Z",
     "iopub.status.busy": "2021-09-28T05:14:51.121191Z",
     "iopub.status.idle": "2021-09-28T05:14:54.425717Z",
     "shell.execute_reply": "2021-09-28T05:14:54.424667Z",
     "shell.execute_reply.started": "2021-09-28T05:14:51.121454Z"
    }
   },
   "outputs": [],
   "source": [
    "#------------------------------\n",
    "# parsing tfrecords basic\n",
    "#------------------------------\n",
    "def data_input_fn(recs,mode): \n",
    "    '''\n",
    "      This Function generates data from gcs\n",
    "      * The parser function should look similiar now because of datasetEDA\n",
    "    '''\n",
    "    def _parser(example):   \n",
    "        feature ={  'image'  : tf.io.FixedLenFeature([],tf.string) ,\n",
    "                    'ulabel'  : tf.io.FixedLenFeature([pos_max],tf.int64),\n",
    "                    'glabel'  : tf.io.FixedLenFeature([pos_max],tf.int64),\n",
    "                    'mask'   : tf.io.FixedLenFeature([mask_len],tf.int64)\n",
    "        }    \n",
    "        parsed_example=tf.io.parse_single_example(example,feature)\n",
    "        # image\n",
    "        image_raw=parsed_example['image']\n",
    "        image=tf.image.decode_png(image_raw,channels=nb_channels)\n",
    "        image=tf.cast(image,tf.float32)/255.0\n",
    "        image=tf.reshape(image,(img_height,img_width,nb_channels))\n",
    "        \n",
    "        # label\n",
    "        label=parsed_example['glabel']\n",
    "            \n",
    "        # position\n",
    "        pos=tf.range(0,pos_max)\n",
    "        pos=tf.cast(pos,tf.int32)\n",
    "        # mask\n",
    "        mask=parsed_example['mask']\n",
    "        mask=1-tf.cast(mask,tf.float32)\n",
    "        mask=tf.stack([mask for _ in range(pos_max)])\n",
    "\n",
    "        return {\"image\":image,\"label\":tf.cast(label, tf.int32),\"pos\":pos,\"mask\":mask},tf.cast(label, tf.float32)\n",
    "    \n",
    "      \n",
    "\n",
    "    # fixed code (for almost all tfrec training)\n",
    "    dataset = tf.data.TFRecordDataset(recs)\n",
    "    dataset = dataset.map(_parser)\n",
    "    dataset = dataset.shuffle(2048,reshuffle_each_iteration=True)\n",
    "    dataset = dataset.repeat()\n",
    "    dataset = dataset.batch(BATCH_SIZE,drop_remainder=True)\n",
    "    dataset = dataset.prefetch(tf.data.experimental.AUTOTUNE)\n",
    "    return dataset\n",
    "\n",
    "train_ds  =   data_input_fn(train_recs,\"train\")\n",
    "eval_ds  =   data_input_fn(eval_recs,\"eval\")\n",
    "\n",
    "\n",
    "\n",
    "#------------------------\n",
    "# visualizing data\n",
    "#------------------------\n",
    "\n",
    "\n",
    "print(\"---------------------------------------------------------------\")\n",
    "print(\"visualizing data\")\n",
    "print(\"---------------------------------------------------------------\")\n",
    "for x,y in train_ds.take(1):\n",
    "    data=np.squeeze(x[\"image\"][0])\n",
    "    plt.imshow(data)\n",
    "    plt.show()\n",
    "    print(\"---------------------------------------------------------------\")\n",
    "    print(\"label:\",x[\"label\"][0])\n",
    "    print(\"---------------------------------------------------------------\")\n",
    "    print(\"pos:\",x[\"pos\"][0])\n",
    "    print(\"---------------------------------------------------------------\")\n",
    "    print(\"mask:\",x[\"mask\"][0][0])\n",
    "    print(\"---------------------------------------------------------------\")\n",
    "    print('Image Batch Shape:',x[\"image\"].shape)\n",
    "    print('Label Batch Shape:',x[\"label\"].shape)\n",
    "    print('Position Batch Shape:',x[\"pos\"].shape)\n",
    "    print('Mask Batch Shape:',x[\"mask\"].shape)\n",
    "    print(\"---------------------------------------------------------------\")\n",
    "    print('Target Batch Shape:',y.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-09-28T05:15:44.226471Z",
     "iopub.status.busy": "2021-09-28T05:15:44.226143Z",
     "iopub.status.idle": "2021-09-28T05:15:45.550522Z",
     "shell.execute_reply": "2021-09-28T05:15:45.549622Z",
     "shell.execute_reply.started": "2021-09-28T05:15:44.226442Z"
    }
   },
   "outputs": [],
   "source": [
    "#-----------------------------------\n",
    "# creating Embedding Weights\n",
    "#-----------------------------------\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "seq_emb              = nn.Embedding(pad_value+2,enc_filters, padding_idx=pad_value)\n",
    "seq_emb_weight       = seq_emb.weight.data.numpy()\n",
    "print(seq_emb_weight.shape)\n",
    "pos_emb              = nn.Embedding(pos_max+1,enc_filters)\n",
    "pos_emb_weight       = pos_emb.weight.data.numpy()\n",
    "print(pos_emb_weight.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-09-28T05:15:51.222607Z",
     "iopub.status.busy": "2021-09-28T05:15:51.221784Z",
     "iopub.status.idle": "2021-09-28T05:15:51.231817Z",
     "shell.execute_reply": "2021-09-28T05:15:51.230819Z",
     "shell.execute_reply.started": "2021-09-28T05:15:51.22257Z"
    }
   },
   "outputs": [],
   "source": [
    "class DotAttention(tf.keras.layers.Layer):\n",
    "    \"\"\"\n",
    "        Calculate the attention weights.\n",
    "        q, k, v must have matching leading dimensions.\n",
    "        k, v must have matching penultimate dimension, i.e.: seq_len_k = seq_len_v.\n",
    "        The mask has different shapes depending on its type(padding or look ahead)\n",
    "        but it must be broadcastable for addition.\n",
    "\n",
    "        Args:\n",
    "        q: query shape == (..., seq_len_q, depth)\n",
    "        k: key shape == (..., seq_len_k, depth)\n",
    "        v: value shape == (..., seq_len_v, depth_v)\n",
    "        mask: Float tensor with shape broadcastable\n",
    "              to (..., seq_len_q, seq_len_k). Defaults to None.\n",
    "\n",
    "        Returns:\n",
    "        output\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.inf_val=-1e9\n",
    "        \n",
    "    def call(self,q, k, v, mask):\n",
    "        matmul_qk = tf.matmul(q, k, transpose_b=True)  # (..., seq_len_q, seq_len_k)\n",
    "       \n",
    "        # scale matmul_qk\n",
    "        dk = tf.cast(tf.shape(k)[-1], tf.float32)\n",
    "        scaled_attention_logits = matmul_qk / tf.math.sqrt(dk)\n",
    "\n",
    "        # add the mask to the scaled tensor.\n",
    "        if mask is not None:\n",
    "            scaled_attention_logits += (mask * self.inf_val)\n",
    "\n",
    "        # softmax is normalized on the last axis (seq_len_k) so that the scores\n",
    "        # add up to 1.\n",
    "        attention_weights = tf.nn.softmax(scaled_attention_logits, axis=-1)  # (..., seq_len_q, seq_len_k)\n",
    "\n",
    "        output = tf.matmul(attention_weights, v)  # (..., seq_len_q, depth_v)\n",
    "\n",
    "        return output\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-09-28T05:16:38.452026Z",
     "iopub.status.busy": "2021-09-28T05:16:38.451422Z",
     "iopub.status.idle": "2021-09-28T05:16:59.830232Z",
     "shell.execute_reply": "2021-09-28T05:16:59.829324Z",
     "shell.execute_reply.started": "2021-09-28T05:16:38.451991Z"
    }
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf \n",
    "\n",
    "\n",
    "def encoder():\n",
    "    '''\n",
    "    creates the encoder part:\n",
    "    * defatult backbone : DenseNet121 **changeable\n",
    "    args:\n",
    "      img           : input image layer\n",
    "        \n",
    "    returns:\n",
    "      enc           : channel reduced feature layer\n",
    "\n",
    "    '''\n",
    "    # img input\n",
    "    img=tf.keras.Input(shape=(img_height,img_width,nb_channels),name='image')\n",
    "    # backbone\n",
    "    backbone=tf.keras.applications.DenseNet121(input_tensor=img ,weights=None,include_top=False)\n",
    "    # feat_out\n",
    "    enc=backbone.output\n",
    "    # enc \n",
    "    #enc=tf.keras.layers.Conv2D(enc_filters,kernel_size=3,activation=\"relu\",padding=\"same\")(enc)\n",
    "    return tf.keras.Model(inputs=img,outputs=enc,name=\"rs_encoder\")\n",
    "\n",
    "def seq_decoder():\n",
    "    '''\n",
    "    sequence attention decoder (for training)\n",
    "    Tensorflow implementation of : \n",
    "    https://github.com/open-mmlab/mmocr/blob/main/mmocr/models/textrecog/decoders/sequence_attention_decoder.py\n",
    "    '''\n",
    "    # label input\n",
    "    gt=tf.keras.Input(shape=(pos_max,),dtype='int32',name=\"label\")\n",
    "    # mask\n",
    "    mask=tf.keras.Input(shape=(pos_max,mask_len),dtype='float32',name=\"mask\")\n",
    "    # encoder\n",
    "    enc=tf.keras.Input(shape=enc_shape,name='enc_seq')\n",
    "    \n",
    "    # embedding\n",
    "    embedding=tf.keras.layers.Embedding(pad_value+2,enc_filters,weights=[seq_emb_weight])(gt)\n",
    "    # sequence layer (2xlstm)\n",
    "    lstm=tf.keras.layers.LSTM(enc_filters,return_sequences=True)(embedding)\n",
    "    query=tf.keras.layers.LSTM(enc_filters,return_sequences=True)(lstm)\n",
    "    # attention modeling\n",
    "    # value\n",
    "    bs,h,w,nc=enc.shape\n",
    "    value=tf.keras.layers.Reshape((h*w,nc))(enc)\n",
    "    attn=DotAttention()(query,value,value,mask)\n",
    "    return tf.keras.Model(inputs=[gt,enc,mask],outputs=attn,name=\"rs_seq_decoder\")\n",
    " \n",
    "\n",
    "\n",
    "def pos_decoder():\n",
    "    '''\n",
    "    position attention decoder (for training)\n",
    "    Tensorflow implementation of : \n",
    "    https://github.com/open-mmlab/mmocr/blob/main/mmocr/models/textrecog/decoders/position_attention_decoder.py\n",
    "    '''\n",
    "    # pos input\n",
    "    pt=tf.keras.Input(shape=(pos_max,),dtype='int32',name=\"pos\")\n",
    "    # mask\n",
    "    mask=tf.keras.Input(shape=(pos_max,mask_len),dtype='float32',name=\"mask\")\n",
    "    # encoder\n",
    "    enc=tf.keras.Input(shape=enc_shape,name='enc_pos')\n",
    "    \n",
    "    # embedding\n",
    "    query=tf.keras.layers.Embedding(pos_max+1,enc_filters,weights=[pos_emb_weight])(pt)\n",
    "    # part-1:position_aware_module\n",
    "    bs,h,w,nc=enc.shape\n",
    "    value=tf.keras.layers.Reshape((h*w,nc))(enc)\n",
    "    # sequence layer (2xlstm)\n",
    "    lstm=tf.keras.layers.LSTM(enc_filters,return_sequences=True)(value)\n",
    "    x=tf.keras.layers.LSTM(enc_filters,return_sequences=True)(lstm)\n",
    "    x=tf.keras.layers.Reshape((h,w,nc))(x)\n",
    "    # mixer\n",
    "    x=tf.keras.layers.Conv2D(enc_filters,kernel_size=3,padding=\"same\")(x)\n",
    "    x=tf.keras.layers.Activation(\"relu\")(x)\n",
    "    key=tf.keras.layers.Conv2D(enc_filters,kernel_size=3,padding=\"same\")(x)\n",
    "    bs,h,w,c=key.shape\n",
    "    key=tf.keras.layers.Reshape((h*w,nc))(key)\n",
    "    attn=DotAttention()(query,key,value,mask)\n",
    "    return tf.keras.Model(inputs=[pt,enc,mask],outputs=attn,name=\"rs_pos_decoder\")\n",
    "\n",
    "def fusion():\n",
    "    '''\n",
    "    fuse the output of gt_attn and pt_attn \n",
    "    '''\n",
    "    # label input\n",
    "    gt_attn=tf.keras.Input(shape=attn_shape,name=\"gt_attn\")\n",
    "    # pos input\n",
    "    pt_attn=tf.keras.Input(shape=attn_shape,name=\"pt_attn\")\n",
    "    \n",
    "    x=tf.keras.layers.Concatenate()([gt_attn,pt_attn])\n",
    "    # Linear\n",
    "    x=tf.keras.layers.Dense(enc_filters*2,activation=None)(x)\n",
    "    # GLU\n",
    "    xl=tf.keras.layers.Activation(\"linear\")(x)\n",
    "    xs=tf.keras.layers.Activation(\"sigmoid\")(x)\n",
    "    x =tf.keras.layers.Multiply()([xl,xs])\n",
    "    # prediction\n",
    "    x=tf.keras.layers.Dense(pad_value+1,activation=None)(x)\n",
    "    return tf.keras.Model(inputs=[gt_attn,pt_attn],outputs=x,name=\"rs_fusion\")\n",
    "\n",
    "with strategy.scope():\n",
    "    rs_encoder    =  encoder()\n",
    "    rs_seq_decoder=  seq_decoder()\n",
    "    rs_pos_decoder=  pos_decoder()\n",
    "    rs_fusion     =  fusion()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-09-28T05:16:59.845081Z",
     "iopub.status.busy": "2021-09-28T05:16:59.8444Z",
     "iopub.status.idle": "2021-09-28T05:16:59.865268Z",
     "shell.execute_reply": "2021-09-28T05:16:59.864346Z",
     "shell.execute_reply.started": "2021-09-28T05:16:59.845051Z"
    }
   },
   "outputs": [],
   "source": [
    "with strategy.scope():\n",
    "    # optimizer\n",
    "    optimizer = tf.keras.optimizers.Adam(lr=0.0001)\n",
    "    # loss\n",
    "    loss_object = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True, reduction='none')\n",
    "    def CE_loss(real, pred):\n",
    "        mask = tf.math.logical_not(tf.math.equal(real, pad_value))\n",
    "        loss_ = loss_object(real, pred)\n",
    "        mask = tf.cast(mask, dtype=loss_.dtype)\n",
    "        loss_ *= mask\n",
    "        return tf.reduce_sum(loss_)/tf.reduce_sum(mask)\n",
    "    def C_acc(real, pred):\n",
    "        accuracies = tf.equal(tf.cast(real,tf.int64), tf.argmax(pred, axis=2))\n",
    "        mask = tf.math.logical_not(tf.math.equal(real, pad_value))\n",
    "        accuracies = tf.math.logical_and(mask, accuracies)\n",
    "        accuracies = tf.cast(accuracies, dtype=tf.float32)\n",
    "        mask = tf.cast(mask, dtype=tf.float32)\n",
    "        return tf.reduce_sum(accuracies)/tf.reduce_sum(mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-09-28T05:16:59.867812Z",
     "iopub.status.busy": "2021-09-28T05:16:59.867565Z",
     "iopub.status.idle": "2021-09-28T05:16:59.893717Z",
     "shell.execute_reply": "2021-09-28T05:16:59.892851Z",
     "shell.execute_reply.started": "2021-09-28T05:16:59.867787Z"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "class robust_scanner(tf.keras.Model):\n",
    "    def __init__(self,encoder,seq_decoder,pos_decoder,fusion):\n",
    "        super(robust_scanner, self).__init__()\n",
    "        self.encoder     = encoder\n",
    "        self.seq_decoder = seq_decoder\n",
    "        self.pos_decoder = pos_decoder\n",
    "        self.fusion      = fusion\n",
    "        \n",
    "    def compile(self,optimizer,loss_fn,acc):\n",
    "        super(robust_scanner, self).compile()\n",
    "        self.optimizer = optimizer\n",
    "        self.loss_fn   = loss_fn\n",
    "        self.acc       = acc\n",
    "       \n",
    "        \n",
    "    def train_step(self, batch_data):\n",
    "        data,gt= batch_data\n",
    "        image=data[\"image\"]\n",
    "        pos  =data[\"pos\"]\n",
    "        mask =data[\"mask\"]\n",
    "        # label\n",
    "        label=tf.ones_like(gt,dtype=tf.float32)*start_end\n",
    "        preds=[]\n",
    "        \n",
    "        with tf.GradientTape() as enc_tape, tf.GradientTape() as pos_dec_tape,tf.GradientTape() as seq_dec_tape,tf.GradientTape() as fusion_tape:\n",
    "            enc    = self.encoder(image, training=True)\n",
    "            pt_attn= self.pos_decoder({\"pos\":pos,\"enc_pos\":enc,\"mask\":mask},training=True)\n",
    "            \n",
    "            gt_attn= self.seq_decoder({\"label\":gt,\"enc_seq\":enc,\"mask\":mask},training=True)\n",
    "            pred   = self.fusion({\"gt_attn\":gt_attn,\"pt_attn\":pt_attn},training=True)\n",
    "            \n",
    "            # loss\n",
    "            loss = self.loss_fn(gt[:,1:],pred[:,:-1,:])\n",
    "            # c acc\n",
    "            char_acc=self.acc(gt[:,1:],pred[:,:-1,:])\n",
    "            \n",
    "        # calc gradients    \n",
    "        enc_grads     = enc_tape.gradient(loss,self.encoder.trainable_variables)\n",
    "        pos_dec_grads = pos_dec_tape.gradient(loss,self.pos_decoder.trainable_variables)\n",
    "        seq_dec_grads = seq_dec_tape.gradient(loss,self.seq_decoder.trainable_variables)\n",
    "        fusion_grads  = fusion_tape.gradient(loss,self.fusion.trainable_variables)\n",
    "        \n",
    "        # apply\n",
    "        self.optimizer.apply_gradients(zip(enc_grads,self.encoder.trainable_variables))\n",
    "        self.optimizer.apply_gradients(zip(pos_dec_grads,self.pos_decoder.trainable_variables))\n",
    "\n",
    "        self.optimizer.apply_gradients(zip(seq_dec_grads,self.seq_decoder.trainable_variables))\n",
    "        self.optimizer.apply_gradients(zip(fusion_grads,self.fusion.trainable_variables))\n",
    "\n",
    "        \n",
    "        return {\"loss\"    : loss,\n",
    "                \"char_acc\": char_acc}\n",
    "    \n",
    "    def test_step(self, batch_data):\n",
    "        data,gt= batch_data\n",
    "        image=data[\"image\"]\n",
    "        pos  =data[\"pos\"]\n",
    "        mask =data[\"mask\"]\n",
    "        # label\n",
    "        label=tf.ones_like(gt,dtype=tf.float32)*start_end\n",
    "        preds=[]\n",
    "        \n",
    "        enc    = self.encoder(image, training=False)\n",
    "        pt_attn= self.pos_decoder({\"pos\":pos,\"enc_pos\":enc,\"mask\":mask},training=False)\n",
    "        \n",
    "        for i in range(pos_max):\n",
    "            gt_attn=self.seq_decoder({\"label\":label,\"enc_seq\":enc,\"mask\":mask},training=False)\n",
    "            step_gt_attn=gt_attn[:,i,:]\n",
    "            step_pt_attn=pt_attn[:,i,:]\n",
    "            pred=self.fusion({\"gt_attn\":step_gt_attn,\"pt_attn\":step_pt_attn},training=False)\n",
    "            preds.append(pred)\n",
    "            # can change on error\n",
    "            char_out=tf.nn.softmax(pred,axis=-1)\n",
    "            max_idx =tf.math.argmax(char_out,axis=-1)\n",
    "            if i < pos_max - 1:\n",
    "                label=tf.unstack(label,axis=-1)\n",
    "                label[i+1]=tf.cast(max_idx,tf.float32)\n",
    "                label=tf.stack(label,axis=-1)\n",
    "                \n",
    "        pred=tf.stack(preds,axis=1)\n",
    "        # loss\n",
    "        loss = self.loss_fn(gt[:,1:],pred[:,:-1,:])\n",
    "        # c acc\n",
    "        char_acc=self.acc(gt[:,1:],pred[:,:-1,:])\n",
    "        \n",
    "        \n",
    "        return {\"loss\"    : loss,\n",
    "                \"char_acc\": char_acc}\n",
    "    \n",
    "   \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-09-28T05:16:59.895327Z",
     "iopub.status.busy": "2021-09-28T05:16:59.895092Z",
     "iopub.status.idle": "2021-09-28T05:16:59.97304Z",
     "shell.execute_reply": "2021-09-28T05:16:59.972083Z",
     "shell.execute_reply.started": "2021-09-28T05:16:59.895302Z"
    }
   },
   "outputs": [],
   "source": [
    "with strategy.scope():\n",
    "    model = robust_scanner(rs_encoder,\n",
    "                           rs_seq_decoder,\n",
    "                           rs_pos_decoder,\n",
    "                           rs_fusion)\n",
    "\n",
    "    model.compile(optimizer = optimizer,\n",
    "                  loss_fn   = CE_loss,\n",
    "                  acc       = C_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-09-28T05:17:01.497513Z",
     "iopub.status.busy": "2021-09-28T05:17:01.497124Z",
     "iopub.status.idle": "2021-09-28T05:17:01.509279Z",
     "shell.execute_reply": "2021-09-28T05:17:01.508076Z",
     "shell.execute_reply.started": "2021-09-28T05:17:01.497399Z"
    }
   },
   "outputs": [],
   "source": [
    "# reduces learning rate on plateau\n",
    "lr_reducer = tf.keras.callbacks.ReduceLROnPlateau(factor=0.1,\n",
    "                                                  cooldown= 10,\n",
    "                                                  patience=10,\n",
    "                                                  verbose =1,\n",
    "                                                  min_lr=0.1e-7)\n",
    "# early stopping\n",
    "early_stopping = tf.keras.callbacks.EarlyStopping(patience=15, \n",
    "                                                  verbose=1, \n",
    "                                                  mode = 'auto') \n",
    "\n",
    "\n",
    "class SaveBestModel(tf.keras.callbacks.Callback):\n",
    "    def __init__(self):\n",
    "        self.best = float('inf')\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        metric_value = logs['val_loss']\n",
    "        if metric_value < self.best:\n",
    "            print(f\"Loss Improved epoch:{epoch} from {self.best} to {metric_value}\")\n",
    "            self.best = metric_value\n",
    "            self.model.encoder.save_weights(\"enc.h5\")\n",
    "            self.model.seq_decoder.save_weights(\"seq.h5\")\n",
    "            self.model.pos_decoder.save_weights(\"pos.h5\")\n",
    "            self.model.fusion.save_weights(f\"fuse.h5\")\n",
    "            print(\"Saved Best Weights\")\n",
    "    def set_model(self, model):\n",
    "        self.model = model\n",
    "            \n",
    "model_save=SaveBestModel()\n",
    "model_save.set_model(model)\n",
    "callbacks= [lr_reducer,early_stopping,model_save]\n",
    "                 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-09-28T05:17:07.231849Z",
     "iopub.status.busy": "2021-09-28T05:17:07.231544Z"
    }
   },
   "outputs": [],
   "source": [
    "EPOCHS=50\n",
    "history=model.fit(train_ds,\n",
    "                  epochs=EPOCHS,\n",
    "                  steps_per_epoch=STEPS_PER_EPOCH,\n",
    "                  verbose=1,\n",
    "                  validation_data=eval_ds,\n",
    "                  validation_steps=EVAL_STEPS, \n",
    "                  callbacks=callbacks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "curves={}\n",
    "for key in history.history.keys():\n",
    "    curves[key]=history.history[key]\n",
    "curves=pd.DataFrame(curves)\n",
    "curves.to_csv(\"history.csv\",index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tfgpu",
   "language": "python",
   "name": "tfgpu"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
